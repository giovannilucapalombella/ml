{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Prontuario per la PI di ML - Giovanni Luca Palombella**"
      ],
      "metadata": {
        "id": "-hJHPpECRpTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Classico - `scikit-learn`"
      ],
      "metadata": {
        "id": "N9PvXtelSHUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1964BhFJIc5vf73IL4uUU7LVVM-CWHa6j?authuser=1#scrollTo=pQnjQVP9K2Vs\n",
        "\n",
        "consulta questo further information"
      ],
      "metadata": {
        "id": "_Q7eUNkPXBqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `LocalOutlierFactor`\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "n_neighbors=20 # iperparametro del LocalOutlierFactor (funziona come un K-NN)\n",
        "\n",
        "data=df.copy() # copia del mio dataset\n",
        "clf=LocalOutlierFactor(n_neighbors=n_neighbors) # unico iperparametro da settare\n",
        "out_in=clf.fit_predict(data[features[:-1]].to_numpy()) # non gli passo le label\n",
        "count_outliers=np.sum(out_in==-1)\n",
        "out_index=np.where(out_in==-1)[0]\n",
        "clean = data[~data.index.isin(out_index)] # rimuovi gli outlier certi [a rigore potrei anche impostare una soglia di tolleranza]\n",
        "sns.pairplot(clean) # plot\n",
        "plt.show()\n",
        "\n",
        "print(\"Nel dataset sono stati trovati\",count_outliers,\"outliers.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q-TDim8kTsGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature importance con Random Forest e XGBoost - supervised [devo avere le label]\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# RandomForest\n",
        "forest = RandomForestRegressor(n_estimators=250, random_state=0)\n",
        "forest.fit(X_train, Y_train)\n",
        "importances_rf = forest.feature_importances_\n",
        "std_rf = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "indices_rf = np.argsort(importances_rf)[::-1]\n",
        "\n",
        "# XGBoost\n",
        "model_xgb = XGBRegressor()\n",
        "model_xgb.fit(X_train, Y_train)\n",
        "importances_xgb = model_xgb.feature_importances_\n",
        "indices_xgb = np.argsort(importances_xgb)[::-1]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Importanza delle feature - Random Forest vs XGBoost\")\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(X.shape[1])\n",
        "\n",
        "bars_rf = plt.bar(index, importances_rf[indices_rf], bar_width, color='r', yerr=std_rf[indices_rf], alpha=1, label='Random Forest')\n",
        "bars_xgb = plt.bar(index + bar_width + 0.1, importances_xgb[indices_rf], bar_width, color='b', alpha=1, label='XGBoost')\n",
        "\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importanza')\n",
        "plt.xticks(index + bar_width / 2, indices_rf)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "for bars in [bars_rf, bars_xgb]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height, '{:.3f}'.format(height), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6gpHxCXXVYA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title KNN\n",
        "importances_cut=importances_rf[:2]\n",
        "def weighted_distance(a,b): # metrica personalizata pesata con la feature importance\n",
        "  dist = 0\n",
        "  for i in range(len(a)):\n",
        "    dist += importances_cut[i]*(a[i]-b[i])**2\n",
        "  return np.sqrt(dist)\n",
        "\n",
        "k_values = [ i + 1 for i in range(15)]\n",
        "best_k = None\n",
        "scores_train = []\n",
        "scores_vali = []\n",
        "ms = 0\n",
        "\n",
        "for k in k_values: # qui c'è il cuore dell'allenamento\n",
        "    model = KNeighborsRegressor(n_neighbors=k , metric=weighted_distance)\n",
        "    model.fit(X_train_norm_cut, Y_train)\n",
        "    scores_train.append(model.score(X_train_norm_cut,Y_train))\n",
        "    scores_vali.append(model.score(X_vali_norm_cut, Y_vali))\n",
        "    if model.score(X_vali_norm_cut, Y_vali) > ms:\n",
        "      ms = model.score(X_vali_norm_cut, Y_vali)\n",
        "      best_k = k\n",
        "\n",
        "print(\"Il miglior valore di k è:\", best_k)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "KNN = KNeighborsRegressor(n_neighbors=best_k,metric=weighted_distance)\n",
        "KNN.fit(X_train_norm_cut, Y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "time_KNN = end_time - start_time\n",
        "\n",
        "plt.title('Scelta del parametro k')\n",
        "plt.scatter(k_values , scores_train , label='train set',color=\"red\")\n",
        "plt.scatter(k_values , scores_vali, label='validation set',color=\"blue\")\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('$R^2$ score')\n",
        "plt.grid(color='gray', linewidth=0.5)\n",
        "plt.xticks(k_values)\n",
        "plt.legend()\n",
        "plt.savefig(\"KNN.png\", dpi=500)\n",
        "plt.show()\n",
        "\n",
        "predicted = KNN.predict(X_test_norm_cut)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.scatterplot(x=predicted, y=Y_test, color='skyblue' , s=10)\n",
        "plt.title(\"Confronto tra Valori Predetti e Valori Reali\")\n",
        "plt.plot(np.linspace(0,3.7,100),np.linspace(0,3.7,100),linestyle='--')\n",
        "plt.xlabel(\"Valori Predetti\")\n",
        "plt.ylabel(\"Valori Reali\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,7))\n",
        "error = Y_test - predicted\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(error, kde=False, color='skyblue', bins=optbins(error))\n",
        "mean_error = np.mean(error)\n",
        "std_error = np.std(error)\n",
        "plt.axvline(x=mean_error, color='red', linestyle='--', label='Media')\n",
        "plt.axvline(x=mean_error + std_error, color='orange', linestyle='--', label='Media + Std')\n",
        "plt.axvline(x=mean_error - std_error, color='orange', linestyle='--', label='Media - Std')\n",
        "plt.title('Distribuzione dell\\'errore di previsione')\n",
        "plt.xlabel('Errore')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Il punteggio sul test set è di\", KNN.score(X_test_norm_cut,Y_test))\n",
        "print(\"Il tempo di esecuzione è di\", time_KNN)\n",
        "print(\"MSE = \", mse(Y_test,predicted))\n",
        "\n",
        "error_KNN = error\n",
        "best_k_KNN = best_k\n",
        "scores_vali_KNN = scores_vali\n",
        "scores_train_KNN= scores_train\n",
        "predicted_KNN = predicted"
      ],
      "metadata": {
        "cellView": "form",
        "id": "If1cnePBWK1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clustering using Gaussian Mixture\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)\n",
        "gmm.fit(data_norm[features].values)\n",
        "\n",
        "y_pred = gmm.predict(data_norm[features].values)\n",
        "\n",
        "#procedura: grid nello spazio PCA e poi trasformazione PCA inversa per\n",
        "#tornare nello spazio originale al quale applicare la predizione della gmm\n",
        "h = .02\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-4., 4., h),\n",
        "                     np.arange(-4., 4., h))\n",
        "\n",
        "zz = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "zz_orig = pca.inverse_transform(zz)\n",
        "prob = gmm.predict_proba(zz_orig)\n",
        "\n",
        "prob = prob.reshape((xx.shape[0],xx.shape[1],3))\n",
        "\n",
        "plt.contour(xx, yy, prob[:,:,0], levels=[0.68], colors='blue')\n",
        "plt.contour(xx, yy, prob[:,:,1], levels=[0.68], colors='brown')\n",
        "plt.contour(xx, yy, prob[:,:,2], levels=[0.68], colors='green')\n",
        "\n",
        "plt.scatter(pca_result[:,0], pca_result[:,1], c=y_pred, cmap='tab10')\n",
        "plt.xlabel('PCA_0')\n",
        "plt.ylabel('PCA_1')\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XrYNaF51gkWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks - `pyTorch`"
      ],
      "metadata": {
        "id": "IReVr4LDSLpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import di pyTorch\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "vA_RJDNlSV0e",
        "outputId": "27b7b411-1d86-4af0-efab-0880068335e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "0.18.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU e set device\n",
        "# controlliamo se la GPU è disponibile e nel caso quale tipo di GPU\n",
        "if torch.cuda.is_available():\n",
        "  print('Numero di GPU disponibili: ',torch.cuda.device_count())\n",
        "  for i in range(0,torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(i))\n",
        "\n",
        "# se la GPU è disponibile setto device='cuda', altrimenti 'cpu\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hCOwb-DyY1Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title From numPy to Torch\n",
        "\n",
        "X_train_pt = torch.from_numpy(X_train).float()\n",
        "Y_train_pt = torch.from_numpy(Y_train).int() # le label sono dei tipi interi\n",
        "X_test_pt = torch.from_numpy(X_test).float()\n",
        "Y_test_pt = torch.from_numpy(Y_test).int()\n",
        "\n",
        "# reshape the tensori da (n,784) a (n,1,28,28) come richiesto da conv layer in pytorch - sono una caterva ['lunga lista'] di matrici 28x28x1 IMMAGINA UNA LISTA DI MATRICI\n",
        "\n",
        "X_train_pt = X_train_pt.reshape(len(X_train_pt), 1, 28,28) #\n",
        "X_test_pt = X_test_pt.reshape(X_test_pt.shape[0], 1, 28, 28) # questa è la soluzione di Giagu\n",
        "\n",
        "print(X_train_pt.shape)\n",
        "print(Y_train_pt.shape)\n",
        "print(X_test_pt.shape)\n",
        "print(Y_test_pt.shape)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vuLfbBz8ZGzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Saver per checkpoint\n",
        "class SaveBestModel:\n",
        "    def __init__(self, best_valid_loss=float('inf')):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "\n",
        "    def __call__(\n",
        "      self, current_valid_loss,\n",
        "      epoch, model, optimizer, criterion):\n",
        "      #Se non vogliamo usare il validation salviamo il modello ogni 5 epoche (eventualmente si può cambiare il numero)\n",
        "      #Lo tengo perchè la validation loss diminuisce ad ogni epoca\n",
        "      if epoch%5:\n",
        "        return 0\n",
        "\n",
        "      if current_valid_loss < self.best_valid_loss:\n",
        "          self.best_valid_loss = current_valid_loss\n",
        "\n",
        "          print(f\"\\n[+] Best validation loss: {self.best_valid_loss}\")\n",
        "          print(f\"[->] Saving best model for epoch: {epoch+1}\")\n",
        "\n",
        "          torch.save({'model' : model,\n",
        "              'epoch': epoch+1,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': criterion,\n",
        "              }, 'best_model.pt')\n",
        "\n",
        "saver = SaveBestModel() # così va istanziato e poi lo chiamo ogni volta all'interno del training loop"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zfiRmEGMhuh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training loop\n",
        "import time\n",
        "\n",
        "num_epochs = 20\n",
        "l1_lambda = 0.0001 # iperparametro del regolarizzatore LASSO - va bene circa un ordine in meno del learning rate\n",
        "hist_loss = []\n",
        "hist_vali_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  t0 = time.time()\n",
        "\n",
        "  #Training\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  counter = 0\n",
        "  for x,_ in dataloaders['train']:\n",
        "    #Spostiamo il tensore sul device\n",
        "    x = x.to(device)\n",
        "\n",
        "    #Creiamo la ricostruzione\n",
        "    output = model(x)\n",
        "\n",
        "    #Aggiungiamo regolarizzione L1 Penalty\n",
        "    l1_penalty = 0\n",
        "    for param in model.parameters():\n",
        "      l1_penalty += torch.sum(torch.abs(param))\n",
        "\n",
        "    #Calcoliamo e aggiorniamo la loss per questa epoca\n",
        "    temp_loss = Loss_fn(output, x)+l1_penalty*l1_lambda # qui calcola la loss\n",
        "    train_loss += temp_loss.item()\n",
        "    counter+=1\n",
        "\n",
        "    #Backpropagation [FA LE DERIVATE]\n",
        "    opt.zero_grad()\n",
        "    temp_loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "  #Salviamo la loss di questa epoca\n",
        "  train_loss /= counter\n",
        "  hist_loss.append(train_loss)\n",
        "\n",
        "  '''\n",
        "  #Validation\n",
        "  vali_loss = 0\n",
        "  counter = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad(): # importantissimo!\n",
        "    for x, _ in dataloaders['test']:\n",
        "      #Spostiamo il tensore sul dispositivo in utilizzo\n",
        "      x = x.to(device)\n",
        "\n",
        "      #Ricostruzione dell'input\n",
        "      output = model(x)\n",
        "\n",
        "      #Calcolo della loss\n",
        "      temp_loss = Loss_fn(output, x)\n",
        "      vali_loss += temp_loss.item()\n",
        "      counter+=1\n",
        "\n",
        "  #Aggiornamento della vali loss\n",
        "  vali_loss/=counter\n",
        "  hist_vali_loss.append(vali_loss)\n",
        "  '''\n",
        "\n",
        "  # check point\n",
        "  saver(vali_loss, epoch, model_ae, opt, Loss_fn)\n",
        "  #         ^---------Va cambiato in train_loss in caso niente validation\n",
        "\n",
        "  #Update dello scheduler - nel caso sia presente\n",
        "  scheduler.step()\n",
        "\n",
        "  #Calcolo del tempo impiegato\n",
        "  elapsed_time = time.time() - t0\n",
        "\n",
        "  #Logging del risultato\n",
        "  print(f'[+] epoch : {epoch+1:4d} | Time : {elapsed_time:.4f} | Train Loss : {train_loss:.8f} | Vali Loss : {vali_loss:.8f}')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Lr2NDZUog9Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Misc"
      ],
      "metadata": {
        "id": "5rpOabvwTmCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Come leggere database *.csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nomefile = '/content/sample_data/mnist_test.csv' # inserire qui il percorso del file\n",
        "df = pd.read_csv(nomefile)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wyej7_uxSVc0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funzione per numero ottimale di bins\n",
        "def optbins(lista):\n",
        "  range=np.max(lista)-np.min(lista)\n",
        "  return int(range*(len(lista)**(1./3.))/(3.49*np.std(lista)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RyET0l31Tm7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split in T-V-T set - *allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes*\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_to_rest_ratio=0.6 # training sample 60%\n",
        "vali_to_test_ratio=0.5  # validation and test sets: 50%-50% of remainining 40%\n",
        "\n",
        "# selezioniamo random data points dai campioni ordinati e disordinati per creare training, test e validation.\n",
        "# shuffle=True dato che i dati sembrano ordinati\n",
        "X_train,X,Y_train,Y=train_test_split(clean[features], clean['target'],train_size=train_to_rest_ratio, shuffle=True, random_state=21)\n",
        "X_vali,X_test,Y_vali,Y_test=train_test_split(X,Y,train_size=vali_to_test_ratio, shuffle=True, random_state=21)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pJnlo6FbUyeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Matrice di correlazione\n",
        "corr_cleaned = clean.corr() # clean è un dataframe di pandas\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(corr_cleaned, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
        "plt.title('Matrice di correlazione')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lhQA-BHrVBMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "confusion_mat = confusion_matrix(y_test.astype(\"int\"), pred)\n",
        "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "POEFsYiTaBvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalizzazione dei sottocampioni - LA NORMALIZZAZIONE VA FATTA INDIPENDENTEMENTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_train_norm = pd.DataFrame(X_train_norm, columns=X_train.columns)\n",
        "\n",
        "X_vali_norm = scaler.transform(X_vali)\n",
        "X_vali_norm = pd.DataFrame(X_vali_norm, columns=X_vali.columns)\n",
        "\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "X_test_norm = pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
        "\n",
        "plt.scatter(X_train_norm['x_0'],X_train_norm['x_1'], label='X_train normalizzato',color=\"blue\")\n",
        "plt.scatter(X_train['x_0'],X_train['x_1'], label='X_train', color='red')\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"$x_0$\")\n",
        "plt.ylabel(\"$x_1$\")\n",
        "plt.title(\"Dati delle feature $x_0$ e $x_1$ prima e dopo la normalizzazione\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "paC-Z9HNXNjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Class balance plot\n",
        "df['class'].value_counts(normalize=True).to_frame().style.bar(color='green')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3VXVsAtGZZjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trasformare label da string a indice numerico intero (0,1,2)\n",
        "from sklearn import preprocessing\n",
        "\n",
        "LE = preprocessing.LabelEncoder() #funzione automatica di scikit learn\n",
        "df['class'] = LE.fit_transform(df['class']) #sostituisce agli elementi della colonna class gli interi che trova in automatico, le etichette le mette lui e non ce ne frega un cazzo\n",
        "df['class'].value_counts()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ua9SvK-4Zp1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COGLIONE!!! PRIMA DI FARE LA PCA DEVI NORMALIZZARE"
      ],
      "metadata": {
        "id": "NCUu0MPFcXv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca2 = PCA(n_components=10)\n",
        "\n",
        "pca2_result = pca2.fit_transform(X_mnist) # dare in input un vettore numpy\n",
        "\n",
        "exp_var2 = pca2.explained_variance_ratio_\n",
        "\n",
        "for i in range(0,len(exp_var2)):\n",
        "  print('explained variance componente: ',i, ' = ', exp_var2[i])\n",
        "print('Explained variance totale delle prime 10 componenti della PCA: {}'.format(sum(exp_var2)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(pca2_result[:,0], pca2_result[:,1], c=Y_mnist, cmap='tab10') # colorare con le label\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Classe')\n",
        "plt.xlabel('$PCA_0$')\n",
        "plt.ylabel('$PCA_1$')\n",
        "\n",
        "\n",
        "# Xtransf_mnist = pca2.inverse_transform(pca2_result[:16]) # trasformazione inversa"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GEJ_LW1Aa5oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esercitazione 5 sulla riduzione dimensionale [TSNE, PCA, uMAP]\n",
        "\n",
        "https://colab.research.google.com/github/stefanogiagu/corso_AI_2024/blob/main/notebooks/es5/MetodiAI_Fis2024_Es5_PCA_tSNE_UMAP.ipynb?authuser=1#scrollTo=BYfpAkW4G44e\n",
        "\n",
        "Oss: uMAP è molto pesante e non preserva le distanze; tuttavia separa molto bene.\n"
      ],
      "metadata": {
        "id": "IbEppELLbyOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella prima parte della esercitazione 6 ci sono degli hints interessanti per quanto riguarda il lavorare con le feature prese da tabelle pandas."
      ],
      "metadata": {
        "id": "Ui3LzVCFcgMv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUkXbi-_cpMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}